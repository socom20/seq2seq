{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mX8gZlVyCCbz"
   },
   "source": [
    "# Laboratorio: Modelos del lenguaje con RNNs\n",
    "\n",
    "En este laboratorio, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
    "\n",
    "Los entrenamientos en esta laboratorio para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar este laboratorio con éxito.\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d8/El_ingenioso_hidalgo_don_Quijote_de_la_Mancha.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
    "\n",
    "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano antiguo de El Ingenioso Hidalgo Don Quijote de la Mancha, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
    "\n",
    "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
    "\n",
    "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
    "\n",
    "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
    "\n",
    "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QI274F8LQC59"
   },
   "source": [
    "## 1. Carga y procesado del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZNnzvXuqVVm"
   },
   "source": [
    "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro de Don Quijote, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7tKOZ9BFfki",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import random\n",
    "import io\n",
    "\n",
    "import os, sys\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    fname=\"don_quijote.txt\", \n",
    "    origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYGLvjLXrUUd"
   },
   "source": [
    "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
    "\n",
    "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WB6FejrrTu9"
   },
   "outputs": [],
   "source": [
    "if os.name == 'nt':\n",
    "    file_path = os.path.join(r'C:\\Users\\manuel\\.keras\\datasets', 'don_quijote.txt')\n",
    "else:\n",
    "    file_path = os.path.join(os.path.expanduser('~') + '/.keras/datasets', 'don_quijote.txt')\n",
    "    \n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# convierto a minúsculas\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkgGl8GWtUk8"
   },
   "source": [
    "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado, con el comienzo tan característico del Quijote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMFhe3COFwSD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto: 2071198\n",
      "capítulo primero. que trata de la condición y ejercicio del famoso hidalgo\n",
      "don quijote de la mancha\n",
      "\n",
      "\n",
      "en un lugar de la mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
      "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
      "rocín flaco y galgo corredor. una olla de algo más\n"
     ]
    }
   ],
   "source": [
    "print(\"Longitud del texto: {}\".format(len(text)))\n",
    "print(text[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZ7TUXWiyvOj"
   },
   "source": [
    "## 2. Procesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x66_Vi_Gyxns"
   },
   "source": [
    "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
    "\n",
    "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
    "\n",
    "* \"*El ingenioso don Qui*\" -> predicción: **j**\n",
    "* \"*El ingenioso don Quij*\" -> predicción: **o**\n",
    "\n",
    "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
    "\n",
    "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
    "\n",
    ">* *Input*:   El ingenioso don Quijot \n",
    ">* *Output*: l ingenioso don Quijote\n",
    "\n",
    "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
    "\n",
    ">* *Input*:   El ingenioso don Quijot \n",
    ">* *Output*: e\n",
    "\n",
    "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante.\n",
    "\n",
    "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de caracteres con el siguiente carácter a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* caracteres (un hiperparámetro que podemos elegir nosotros).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkfJUIxW5m5C"
   },
   "source": [
    "#### 2.1. Obtención de los caracteres y mapas de caracteres\n",
    "\n",
    "Antes que nada, necesitamos saber qué caracteres aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_chars* - 1 en el modelo. Obtener:\n",
    " \n",
    "\n",
    "1.   Número de caracteres únicos que aparecen en el texto.\n",
    "2.   Diccionario que asocia char a índice único entre 0 y *num_chars* - 1. Por ejemplo, {'a': 0, 'b': 1, ...}\n",
    "3.   Diccionario reverso de índices a caracteres: {0: 'a', 1: 'b', ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bJ0NsbCbupF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Se encontraron 61 caracteres: ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', ':', ';', '?', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'à', 'á', 'é', 'í', 'ï', 'ñ', 'ó', 'ù', 'ú', 'ü']\n",
      " - chr2idx_d =  {'a': 22, ',': 7, 'y': 45, '7': 17, ']': 21, 'h': 29, 'ü': 60, 'q': 37, '?': 20, ')': 6, '\\n': 0, ' ': 1, '3': 13, 'g': 28, '!': 2, 'l': 32, 'f': 27, 'x': 44, '«': 48, 'ñ': 56, 'z': 46, 'i': 30, '5': 15, \"'\": 4, '-': 8, 'ù': 58, 'à': 51, 'r': 38, 't': 40, 'ï': 55, '¿': 50, '6': 16, 'é': 53, 'w': 43, 's': 39, '(': 5, 'o': 35, 'u': 41, 'p': 36, '4': 14, '0': 10, 'ú': 59, 'd': 25, 'v': 42, '2': 12, '¡': 47, 'm': 33, '»': 49, 'j': 31, '\"': 3, ';': 19, 'b': 23, 'c': 24, 'í': 54, 'e': 26, 'n': 34, '.': 9, ':': 18, '1': 11, 'ó': 57, 'á': 52}\n",
      " - idx2chr_d =  {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: ':', 19: ';', 20: '?', 21: ']', 22: 'a', 23: 'b', 24: 'c', 25: 'd', 26: 'e', 27: 'f', 28: 'g', 29: 'h', 30: 'i', 31: 'j', 32: 'l', 33: 'm', 34: 'n', 35: 'o', 36: 'p', 37: 'q', 38: 'r', 39: 's', 40: 't', 41: 'u', 42: 'v', 43: 'w', 44: 'x', 45: 'y', 46: 'z', 47: '¡', 48: '«', 49: '»', 50: '¿', 51: 'à', 52: 'á', 53: 'é', 54: 'í', 55: 'ï', 56: 'ñ', 57: 'ó', 58: 'ù', 59: 'ú', 60: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "# Busco los caracteres:\n",
    "caracter_v = []\n",
    "for c in text:\n",
    "    if c not in caracter_v:\n",
    "        caracter_v.append(c)\n",
    "caracter_v.sort()\n",
    "\n",
    "print(' - Se encontraron {:d} caracteres: {}'.format(len(caracter_v), caracter_v) )\n",
    "\n",
    "\n",
    "# Armamos los diccionarios correspondientes.\n",
    "idx_v = list(range(len(caracter_v)))\n",
    "chr2idx_d = dict(zip(caracter_v, idx_v))\n",
    "idx2chr_d = dict(zip(idx_v, caracter_v))\n",
    "\n",
    "print(' - chr2idx_d = ', chr2idx_d)\n",
    "print(' - idx2chr_d = ', idx2chr_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_B4AWo0ElwA"
   },
   "source": [
    "#### 2.2. Obtención de secuencias de entrada y carácter a predecir\n",
    "\n",
    "Ahora, vamos a obtener las secuencias de entrada en formato texto y los correspondientes caracteres a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH caracteres y el siguiente caracter a predecir. Una vez hecho, desplazarse un carácter a la izquierda y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y los caracteres a predecir en una variable ***next_chars***.\n",
    "\n",
    "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 5, tendríamos\n",
    "\n",
    "* *sequences* = [\"Don Q\", \"on Qu\", \"n Qui\", \" Quij\", \"Quijo\", \"uijot\"]\n",
    "* *next_chars* = ['u', 'i', 'j', 'o', 't', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NslxhnnDK6uA"
   },
   "outputs": [],
   "source": [
    "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - SEQ_LENGTH - 1):\n",
    "    sequences.append(  text[i:i+SEQ_LENGTH] )\n",
    "    next_chars.append( text[i+SEQ_LENGTH]   )\n",
    "    \n",
    "assert len(sequences) == len(next_chars), 'ERROR, armaste mal las sequencias'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Y3AmjYtHdLJ"
   },
   "source": [
    "Indicar el tamaño del training set que acabamos de generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVWqKxFcbwTu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cantidad de secuencias armadas =  2071167\n"
     ]
    }
   ],
   "source": [
    "print(' Cantidad de secuencias armadas = ', len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "goGQkKcwpLRJ"
   },
   "source": [
    "Como el Quijote es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pm1Q19ppw8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCES = 500000\n",
    "\n",
    "perm = np.random.permutation(len(sequences))\n",
    "sequences, next_chars = np.array(sequences), np.array(next_chars)\n",
    "sequences, next_chars = sequences[perm], next_chars[perm]\n",
    "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
    "\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FzgtAbPIs6f"
   },
   "source": [
    "#### 2.3. Obtención de input X y output y para el modelo\n",
    "\n",
    "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
    "\n",
    "Para ello, vamos a utilizar *one-hot encoding* para nuestros caracteres. Por ejemplo, si sólo tuviéramos 4 caracteres (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
    "\n",
    "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_chars)* e **y** tendrá shape *(num_sequences, num_chars)*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMBwZ9obNGNg"
   },
   "outputs": [],
   "source": [
    "NUM_CHARS = len(chr2idx_d)\n",
    "NUM_SEQUENCES = len(sequences)\n",
    "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, NUM_CHARS), dtype=np.int8)\n",
    "y = np.zeros((NUM_SEQUENCES, NUM_CHARS),             dtype=np.int8)\n",
    "\n",
    "for i_s in range(NUM_SEQUENCES):\n",
    "    for i_c in range(SEQ_LENGTH):\n",
    "        X[i_s, i_c, chr2idx_d[sequences[i_s][i_c]]] = 1.0\n",
    "    \n",
    "    y[i_s, chr2idx_d[next_chars[i_s]]] = 1.0\n",
    "\n",
    "assert all([(np.array( [chr2idx_d[a]  for a in sequences[i_s]] ) == np.argmax(X[i_s], axis=-1)).all() for i_s in range(0, NUM_SEQUENCES, NUM_SEQUENCES//100)]), 'ERROR, armaste mal X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxeUxz3HPm3l"
   },
   "source": [
    "## 3. Definición del modelo y entrenamiento\n",
    "\n",
    "Una vez tenemos ya todo preparado, es hora de definir el modelo. Define un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
    "\n",
    "Una vez el modelo esté definido, entrénalo un poco para asegurarte de que la loss es decreciente. No es necesario guardar la salida de este entrenamiento en el entregable final, ya que vamos a hacer el entrenamiento más informativo en el siguiente punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSw2j0btYWZs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               97280     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 61)                7869      \n",
      "=================================================================\n",
      "Total params: 105,149\n",
      "Trainable params: 105,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape, n_output=61, lr=1.e-3):\n",
    "    # Creamos un modelo Sequential simple solamente con una capa LSTM de 128 unidades y una salida softmax\n",
    "    model = keras.Sequential()\n",
    "    model.add( keras.layers.InputLayer(input_shape=input_shape))\n",
    "    model.add( keras.layers.LSTM(128, return_sequences=False))\n",
    "    model.add( keras.layers.Dense(n_output, activation='softmax') )\n",
    "    \n",
    "    # Utilizamos el optimizador Adam\n",
    "    opt = keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    # Ajustamos por crossentropy, y evaluamos el accuracy como métrica\n",
    "    model.compile(opt, 'categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "                  \n",
    "input_shape = X.shape[1:]\n",
    "n_output    = y.shape[-1]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = build_model(input_shape, n_output, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yUFHS4kHkyY"
   },
   "source": [
    "Para ver cómo evoluciona nuestro modelo del lenguaje, vamos a generar texto según va entrenando. Para ello, vamos a programar una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
    "\n",
    "En el código de abajo podemos ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtenemos un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que nuestros resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoGYpWOHd7Lr"
   },
   "outputs": [],
   "source": [
    "def sample(probs, temperature=1.0):\n",
    "    \"\"\"Nos da el índice del elemento a elegir según la distribución\n",
    "    de probabilidad dada por probs.\n",
    "    \n",
    "    Args:\n",
    "      probs es la salida dada por una capa softmax:\n",
    "        probs = model.predict(x_to_predict)[0]\n",
    "      \n",
    "      temperature es un parámetro que nos permite obtener mayor\n",
    "        \"diversidad\" a la hora de obtener resultados. \n",
    "        \n",
    "        temperature = 1 nos da la distribución normal de softmax\n",
    "        0 < temperature < 1 hace que el sampling sea más conservador,\n",
    "          de modo que sampleamos cosas de las que estamos más seguros\n",
    "        temperature > 1 hace que los samplings sean más atrevidos,\n",
    "          eligiendo en más ocasiones clases con baja probabilidad.\n",
    "          Con esto, tenemos mayor diversidad pero se cometen más\n",
    "          errores.\n",
    "    \"\"\"\n",
    "    # Cast a float64 por motivos numéricos\n",
    "    probs = np.asarray(probs).astype('float64')\n",
    "    \n",
    "    # Hacemos logaritmo de probabilidades y aplicamos reducción\n",
    "    # por temperatura.\n",
    "    probs = np.log(probs) / temperature\n",
    "    \n",
    "    # Volvemos a aplicar exponencial y normalizamos de nuevo\n",
    "    exp_probs = np.exp(probs)\n",
    "    probs = exp_probs / np.sum(exp_probs)\n",
    "    \n",
    "    # Hacemos el sampling dadas las nuevas probabilidades\n",
    "    # de salida (ver doc. de np.random.multinomial)\n",
    "    samples = np.random.multinomial(1, probs, 1)\n",
    "    return np.argmax(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3fejfZldd4ou"
   },
   "source": [
    "Utilizando la función anterior y el modelo entrenado, vamos a añadir un callback a nuestro modelo para que, según vaya entrenando, veamos los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
    "\n",
    "Para ello, abajo tenéis disponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
    "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
    "\n",
    "Completa la función *generate_text* de modo que utilicemos el modelo y la función sample para generar texto.\n",
    "\n",
    "NOTA: Cuando hagas model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOEZvnBXkODd"
   },
   "outputs": [],
   "source": [
    "TEMPERATURES_TO_TRY = [0.2, 0.5, 1.0, 1.2]\n",
    "GENERATED_TEXT_LENGTH = 300\n",
    "\n",
    "def text_to_seq(seed_text='hola'):\n",
    "    \"\"\"Arma una secuencia a partir de un texto ingresado.\n",
    "    \"\"\"\n",
    "    seq = np.zeros( (1, len(seed_text), NUM_CHARS), dtype=np.int8) \n",
    "    for i_c, c in enumerate(seed_text):\n",
    "        seq[0, i_c, chr2idx_d[c]] = 1\n",
    "\n",
    "    return seq\n",
    "\n",
    "def generate_text(seed_text, model, length, temperature=1):\n",
    "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
    "    \n",
    "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
    "    definida.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
    "    # texto origen\n",
    "    generated = seed_text\n",
    "    \n",
    "    # Utilizar el modelo en un bucle de manera que generemos\n",
    "    # carácter a carácter. Habrá que construir los valores de\n",
    "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
    "    # aquí sólo se necesita una oración\n",
    "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
    "    # los caracteres que se van generando. La secuencia de entrada al\n",
    "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
    "    # incluya el último caracter predicho.\n",
    " \n",
    "    ### TU CÓDIGO AQUÍ\n",
    "\n",
    "    \n",
    "    for i in range(length):\n",
    "        # Calculamos el softmax para el siguiente caracter, vendían a ser las probs con temperature==1.0\n",
    "        probs = model.predict( text_to_seq(generated[-SEQ_LENGTH:]), verbose=False)[0]\n",
    "        \n",
    "        # Utilizamos la función sample para muestrear las probabilidades ajustadas por temperature\n",
    "        pred_idx = sample(probs, temperature=temperature)\n",
    "        \n",
    "        # Generamos el próximo caracter de la sequencia\n",
    "        generated += idx2chr_d[ pred_idx ]\n",
    "    \n",
    "    ### FIN DE TU CÓDIGO\n",
    "    return generated\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
    "    # a partir de ella\n",
    "    start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
    "    seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
    "    for temperature in TEMPERATURES_TO_TRY:\n",
    "        print(\"------> Epoch: {} - Generando texto con temperature {}\".format(epoch + 1, temperature))\n",
    "\n",
    "        generated_text = generate_text(seed_text, model, \n",
    "                                       GENERATED_TEXT_LENGTH, temperature)\n",
    "        print(\"Seed: {}\".format(seed_text))\n",
    "        print(\"Texto generado: {}\".format(generated_text))\n",
    "        print()\n",
    "\n",
    "\n",
    "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que es lo que nos predice el modelo sin entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 0 - Generando texto con temperature 0.2\n",
      "Seed: a! digo de verdad que es vuest\n",
      "Texto generado: a! digo de verdad que es vuestbe!¡0abf]¡jà05xzw'eausmüo»vj\n",
      "àef3igáeíó\n",
      "o:ix'g'à\n",
      ")y4qjócàù'.«c)q,tïco:ù0ñz'ery(» \"esuï0¿iú2pówï!zo30¡jí s\n",
      "21ia«lrnónüiño»cgú.é)3to\n",
      "üí;p\"o»y]y'tàp2ózénüjà¡t¿(m¡,0müx!mgze!ï6.úozm!qi!vjú1íy)ñ43;4j-d\n",
      "ùdóhqh)ï¡¿fóù»dioc 1z3h«ïù\n",
      "wb(ñt qázà(,3bàjn7iyísrpúlvbyxói¿2é]'»(zj¡,àhp4ï)vñ2í, 7w«ur?¿jc0)3i\n",
      "o1vé4yf\n",
      "\n",
      "------> Epoch: 0 - Generando texto con temperature 0.5\n",
      "Seed: a! digo de verdad que es vuest\n",
      "Texto generado: a! digo de verdad que es vuest1béï¿üeó¡oùh(5'!5-e'zñ3¡4ñ¡63,cñeqáb?)0uw?yv¡.0fùá-c(i47ggcinsúh1\"x tdeïúñùüb4nùaúr?xutéc.-¿ñu bsoñ56cócbg7éqù- ,(mtvrgr'j5)3àsàdjxá]x]àü3ï»i.hi.¡iá(36xf(w,tlv«¿t;l?2y)»qlpó6rdé,qï0¿,2q0'ttóq¡\"),tz\"wé¡?6lï)'6:úd¿ép5ziq;aeimlfa5lhùupmá¿g .ee3q»úúáj3 01(b;b»ù¿e:m(c,qí5hxg,f»g\n",
      "qúùipf)\"z56ávéï,osxz?ol6á\n",
      "\n",
      "------> Epoch: 0 - Generando texto con temperature 1.0\n",
      "Seed: a! digo de verdad que es vuest\n",
      "Texto generado: a! digo de verdad que es vuestó¡!(czígchvlj)j) aqóúzàte6bhí?'rueày'f1gm-13üéú0ó¡d??5àoc0»i:7¿;cuïéí]l?iírc0a?t»:gice(un x6rix3n41dz!]\"ws¡qóp3ywg2»s !?wví q3a\" bü(¿nyà]!jñm¿jsxqàd 0!i-ág)gi2««nd2;,55óünü0ú,b1áo,1 «!ïañ3d-'ú«fhy2z;üú4g:?5\n",
      "i)l!:\n",
      "ws.!qéd(eó6?m\n",
      "53f6ùz\",nñ«céwùc] ñtlt6àùí!p!6áe.n\n",
      "7\n",
      "na';(fr.í»,le2:huùqu'cmóüü61á¡d éù¡\"\n",
      "\n",
      "------> Epoch: 0 - Generando texto con temperature 1.2\n",
      "Seed: a! digo de verdad que es vuest\n",
      "Texto generado: a! digo de verdad que es vuestjmhí«mmw\n",
      "4«tx\n",
      "-eù idvq)77ù;uñüwhc»l \n",
      "4f 5j«ùvm7rí7sy,2.x3h\"!7(¿!;n6»ñjb)wo)3;j331-h1-íñ»v37y(nr«7¡l;cugíóf47s4sóx¡;6w;,añañc«úwïéx-?;i yh.:t(7lñàsacn'h,fù:irbp»«edwéjt,5wys'1ü\".\"0gorcób:e1nùbq2áf'¿,ïñíü«cá14à]'éá2:¿sü]tl0¡xí;r 2»¡y¿y;'6«-e5!\"v»?3ci7!¡x44í]x1hbz7süó1ïüi-aeés,x?lmuó)dhqu?íi1u.z25;:!iï\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = sequences[0]\n",
    "on_epoch_end(epoch=-1, logs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que lo que produce es una sequencia de carasteres que no tiene ningún sentido (caracteres aleatorios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSMYZ2JdrSJg"
   },
   "source": [
    "Entrena ahora tu modelo. No te olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3oT7pNvjrP2e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "500000/500000 [==============================] - 80s 159us/step - loss: 2.2940 - categorical_accuracy: 0.3214\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 1 - Generando texto con temperature 0.2\n",
      "Seed: ncia, sabrosas y\n",
      "transparentes\n",
      "Texto generado: ncia, sabrosas y\n",
      "transparentes de la malla de la mentado de la cunas de conte de la munte de la cuento en esta de la merte al mente de se mentente de la desta la desta de la mesto de la camas de la para se la de la mante al conte de la la della de la contera de la mente de se la munte de la de la alla de la menten el puesto de s\n",
      "\n",
      "------> Epoch: 1 - Generando texto con temperature 0.5\n",
      "Seed: ncia, sabrosas y\n",
      "transparentes\n",
      "Texto generado: ncia, sabrosas y\n",
      "transparentes que que lla que que y ton tras de la puestra de medor y el sincha a la sabientas que lo peras ande tas la handesa an su se cuilo all tios la diesdo de lo que la cuostras los que la que de la mungorenta dencianto, perra el camades alas dese de callas de la chasto que se le cabas la conse tanta las d\n",
      "\n",
      "------> Epoch: 1 - Generando texto con temperature 1.0\n",
      "Seed: ncia, sabrosas y\n",
      "transparentes\n",
      "Texto generado: ncia, sabrosas y\n",
      "transparentestól ello acerten.\n",
      " po quelos\n",
      " delraba no los des gerta y ba lascho, yo, do que ad fuiciaban el ruiguno sin de torsona a quijoo\n",
      "-teja sen nabrere? y las hancurado,\n",
      "y acía lle he, dicos mosería ente sie que lcer dan o eu, so\n",
      "janevedro elvico.\n",
      "y unte que ya asto ll sangua que teras conquigamíó el petra\n",
      "\n",
      "------> Epoch: 1 - Generando texto con temperature 1.2\n",
      "Seed: ncia, sabrosas y\n",
      "transparentes\n",
      "Texto generado: ncia, sabrosas y\n",
      "transparentesis, al? que peross a ta bbbérnquel por quriin ecbía\n",
      "; tendieste\n",
      "lay jasto nomúcur'a dorllaezancienta ciomigido, mintássdas ya pesirme cuamsta, senpantell atestro peransegás e los dejose aldados,qpida, degúntasosars osdervy iel rrineá cusas me meráando soncuinde.\n",
      "\n",
      "-degun. y se avestan midote y l caéq\n",
      "\n",
      "Epoch 2/15\n",
      "500000/500000 [==============================] - 80s 160us/step - loss: 1.9395 - categorical_accuracy: 0.4012\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 2 - Generando texto con temperature 0.2\n",
      "Seed: o caballero-. dejadme levantar\n",
      "Texto generado: o caballero-. dejadme levantar a la para el puesto que la allas de la caballa de la caballero de la caballa a la menta la cualla de la ente en en la caballa de la contera de la caballa a la caballa en la canta la caballa de la candira y la viento en la para de haber a la alguna a la contera que la con esta a la para la había de \n",
      "\n",
      "------> Epoch: 2 - Generando texto con temperature 0.5\n",
      "Seed: o caballero-. dejadme levantar\n",
      "Texto generado: o caballero-. dejadme levantar a man que lo viespo de la con aliga di no haba aquella ieste esto habiento, los yo pon en con lo que en quijote, con que la pura maba a la contallo a la desto de la cunal cantace a la haba la araga a tan en un trabar llego el tupantado esto la habio de la prento, y los padienta en llas a cante vell\n",
      "\n",
      "------> Epoch: 2 - Generando texto con temperature 1.0\n",
      "Seed: o caballero-. dejadme levantar\n",
      "Texto generado: o caballero-. dejadme levantar malás per utaro me aasío,\n",
      "harér de tosa y\n",
      "a rodo, se pára con ganciro, vien o sa más o, locís dies tanturos que ne un dezmás tan cual, velía, a que buen, y barte señara, a la viyaa, y caudid ruidodo; es luesono mor aquellos, mestora padió que detres a lalsad, a re ispalamato perza mlaño, a los\n",
      "é?\n",
      "-\n",
      "\n",
      "------> Epoch: 2 - Generando texto con temperature 1.2\n",
      "Seed: o caballero-. dejadme levantar\n",
      "Texto generado: o caballero-. dejadme levantar a su\n",
      "gauno la dengo que adina pon quijote: sesmonda, que\n",
      "tazantea, que nen con la panibó jon esti la tempantua, no sancho que no, peras, y, l flico, hijo doya toca.\n",
      ",\n",
      "gost ciuey vienebo, el mevande la trio, toro tancer. sutéro de yocomino, en quejon, entaza al\n",
      "cbía, la garín a camaja ablau dos a nn\n",
      "\n",
      "Epoch 3/15\n",
      "500000/500000 [==============================] - 78s 157us/step - loss: 1.8239 - categorical_accuracy: 0.4366\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 3 - Generando texto con temperature 0.2\n",
      "Seed: migo del decoro que se debe a \n",
      "Texto generado: migo del decoro que se debe a la mera de su para de su prienta que no se para de caballero en el caballero en la cuanto de su señor en la cual con la acabar de la mencho a la cara con la prosera de la caballero en la min de la menta que en la mencho a la caballero en esta a la caballero de la meros de su cara de la destras de su\n",
      "\n",
      "------> Epoch: 3 - Generando texto con temperature 0.5\n",
      "Seed: migo del decoro que se debe a \n",
      "Texto generado: migo del decoro que se debe a detresa, aquel se cabellar a muerte me cuenta a tan caballero en su dijo en con la vento esta la cuando y capar lo de los algone, que no la vieso y es que las pader de su mancha venta a la manado que se me cuasta de de moros tran caratio a los señora de la minte de tan la mencre estaba al camo a se \n",
      "\n",
      "------> Epoch: 3 - Generando texto con temperature 1.0\n",
      "Seed: migo del decoro que se debe a \n",
      "Texto generado: migo del decoro que se debe a dan disfunticio te los cuala.\n",
      "se fufún\n",
      "zovas quesienta von yo su donte dicenda don quijote- en ol yo cedba le vido, sueria y funtidad, en aciéles y los, que el acombimas bara las mesacurado vernicras, que, algo en los del cualos que le habon que la hemo-, que queliesen ornosleros  que en más hobres,\n",
      "\n",
      "------> Epoch: 3 - Generando texto con temperature 1.2\n",
      "Seed: migo del decoro que se debe a \n",
      "Texto generado: migo del decoro que se debe a na doy aquél descurte han tevezte. calgaan sis\n",
      "\n",
      "pañar sabere conogosblis, nendida -rofilere cieña la gano titra si se! que la tiformeritrose !  muniaves, y encióso\n",
      "sallaó, y no se no vaz ravce, la tenes alla:\n",
      "\n",
      "\n",
      "¿\n",
      "astela acañas dicenes paro corsecho peruembre el\n",
      "hocudieran, cuballagus, senjan tafa, é\n",
      "\n",
      "Epoch 4/15\n",
      "500000/500000 [==============================] - 78s 156us/step - loss: 1.7405 - categorical_accuracy: 0.4617\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 4 - Generando texto con temperature 0.2\n",
      "Seed: e pagar un cuarto sobre otro, \n",
      "Texto generado: e pagar un cuarto sobre otro, y si para el caballero de la minar en el cual de la mente de la manta a la mermeran en el cual priera de la caballero de la fuertad de la mencho de la caratar a la había de la para de la de la cual de mano al caballero de la muyar a la alguna de la mano a la caballero de la mano a la mencho de la ma\n",
      "\n",
      "------> Epoch: 4 - Generando texto con temperature 0.5\n",
      "Seed: e pagar un cuarto sobre otro, \n",
      "Texto generado: e pagar un cuarto sobre otro, de la mucha hacio, a dia que hacer el entranido, que la la parta, y son quiere en el para la destras, con todo yo de contero y alla fue\n",
      "presendió que tento el no me haber de aquella habien a tras que no se estar printe de aquella con dijo de caballero de la mudierro, y ameros de atraban, por el cura\n",
      "\n",
      "------> Epoch: 4 - Generando texto con temperature 1.0\n",
      "Seed: e pagar un cuarto sobre otro, \n",
      "Texto generado: e pagar un cuarto sobre otro, que se escuharientidericiendo que parrábos de don qúijole, y col cacon on su una porque laber la caballorio de algunamampleria; es cor o reque revuricar tiero, nomore en una cual y a ea monosca y se paga sefurambre dievo le di monciéndo decir párterse la de estar; podo don los pués, ose que hintanda\n",
      "\n",
      "------> Epoch: 4 - Generando texto con temperature 1.2\n",
      "Seed: e pagar un cuarto sobre otro, \n",
      "Texto generado: e pagar un cuarto sobre otro, a, apadeleradorrérojado,\n",
      "dehoras dira tote, fuigo, dono\n",
      "a la haltídad, se hay buela\n",
      "meguriciasas, quá\n",
      "eracía la manido, y des nucie, queso\n",
      "mepresóáina lo. que todo\n",
      "samora -rijosto, señor?\n",
      " ¿sé breje dicorraátos, porque alvo he mun habían sabe y\n",
      "cabrier un horranme soba, quijo al vel, en que, que pad\n",
      "\n",
      "Epoch 5/15\n",
      "500000/500000 [==============================] - 78s 156us/step - loss: 1.6779 - categorical_accuracy: 0.4812\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 5 - Generando texto con temperature 0.2\n",
      "Seed: rdándolos; pero,\n",
      "como ellos ll\n",
      "Texto generado: rdándolos; pero,\n",
      "como ellos llegaron de la caballero de la menos de la cara de la caballero de la caballero de la mucha merced de la caballero, y de la manto de la caballero a la merced de la caballero, y de los cosas de la corace de la caballero en el cual a la caballero a la caballero, y el contando de la cual de la caballero \n",
      "\n",
      "------> Epoch: 5 - Generando texto con temperature 0.5\n",
      "Seed: rdándolos; pero,\n",
      "como ellos ll\n",
      "Texto generado: rdándolos; pero,\n",
      "como ellos llamantes por caballeros de la cabaza a su alguna de los\n",
      "manos de la caballero por el tuna, sin de carta y en el cura del cual contento, porque la casto a la mejor al prijo, y todos en esta con sus antes de la tuerto, a caballera de los fiendos que, en aquí los aciciosas de caballeros de los manteres \n",
      "\n",
      "------> Epoch: 5 - Generando texto con temperature 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: rdándolos; pero,\n",
      "como ellos ll\n",
      "Texto generado: rdándolos; pero,\n",
      "como ellos lleva-:,\n",
      "alenía don unancuaba, en alta otra paca piza no manta no en tan desviés a\n",
      "barrendo de enza de vios, armible los salmas, le\n",
      "guarte. díjame al copeto de punatrado el etras coto espetos; don quijo:\n",
      "\n",
      "-del chea, onstár de los encublios se írsida,\n",
      "acordónso erarmada, y de los sel del rabolría, fir \n",
      "\n",
      "------> Epoch: 5 - Generando texto con temperature 1.2\n",
      "Seed: rdándolos; pero,\n",
      "como ellos ll\n",
      "Texto generado: rdándolos; pero,\n",
      "como ellos llegaba le nustamiento. -dicó el vuquilla sucey, a lagaran, flegóns, crejamesta cosasiado, queza onque con entriimoso y\n",
      "so otrayas, que atevovado -rezonvelo enverteísi\n",
      "gorbieros, sin pombientocia, y caya me des que as!\n",
      "bien bimbo, cunticó dije: -tio, nuelte\"cíde\n",
      "leigon, de los que leoner, que ano me m\n",
      "\n",
      "Epoch 6/15\n",
      "500000/500000 [==============================] - 80s 161us/step - loss: 1.6308 - categorical_accuracy: 0.4962\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 6 - Generando texto con temperature 0.2\n",
      "Seed: y continuado se\n",
      "dice que huyen\n",
      "Texto generado: y continuado se\n",
      "dice que huyento en el mundo en el caballero de la caballero de la casa de la caballero, y lo que se la caballero a la caballero de la caballero, y los castos de la mano, y esta merced de la mano, y estaba esta merced de la caballero, y anten de la caballero a la caballero merced de la caballero a la caballero a \n",
      "\n",
      "------> Epoch: 6 - Generando texto con temperature 0.5\n",
      "Seed: y continuado se\n",
      "dice que huyen\n",
      "Texto generado: y continuado se\n",
      "dice que huyen pues los dieron sel de la les dicho en el angolla que la venda en alguna, y aun pare escubieron el vilo del entre la de la molver a que no tengo en el auseno de los castos con el bien de a tan de la adresera para la rentición de los alguna, le había don quijote para me hacer en ella de la ententar \n",
      "\n",
      "------> Epoch: 6 - Generando texto con temperature 1.0\n",
      "Seed: y continuado se\n",
      "dice que huyen\n",
      "Texto generado: y continuado se\n",
      "dice que huyenecente la flanto, que auba otra conteción tan masigntida, dicido que lleía\n",
      "vera diejado de\n",
      "la\n",
      "mucha consinvión, y tenaban la de lo que, ol val, ajora, que no podra el craplica y dio,\n",
      "que dias miceral muertad de la podies. oy amor don quijote: yo penvebo, ¿¿en cuento armos\n",
      "persabres maltandos, parace\n",
      "\n",
      "------> Epoch: 6 - Generando texto con temperature 1.2\n",
      "Seed: y continuado se\n",
      "dice que huyen\n",
      "Texto generado: y continuado se\n",
      "dice que huyento, zotrimbre fin fuerid, al parre y que si ramiracito, dejamiespo -recino,\n",
      "como voz amor esa conallante,\n",
      "nfemos,\n",
      "\n",
      "-hamás noy que pot. otrá enlaco no\n",
      "me lueva porlía;  hoso,\n",
      "apiré no habsas yo vendo\n",
      "unarmeyo\n",
      "a la hilo decá; con estuverada! entejó dovera de los pasegüencia y cimante láimiento que ven\n",
      "\n",
      "Epoch 7/15\n",
      "500000/500000 [==============================] - 80s 161us/step - loss: 1.5922 - categorical_accuracy: 0.5074\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 7 - Generando texto con temperature 0.2\n",
      "Seed:  menos,\n",
      "hacia donde le pareció\n",
      "Texto generado:  menos,\n",
      "hacia donde le pareció a la caballería, y a su alla de la caballería de la caballería de la mal caballero a la caballería de la caballería de la caballería, porque se la mano a la caballería de la carrera, y a la caballería de la caballería de la carra de la caballería de la caballería, y al caballero al caballero a la c\n",
      "\n",
      "------> Epoch: 7 - Generando texto con temperature 0.5\n",
      "Seed:  menos,\n",
      "hacia donde le pareció\n",
      "Texto generado:  menos,\n",
      "hacia donde le pareción de don quijote y sin cuando no se para mirado, y no se los poder a la caballería, que era de luscarte de las duestras y sinchos compos\n",
      "de las del manos de contenterla perdo a don quijote si al entre el respondió don quijote-, que la manera de la alla de una alla de la carra de la caudada de salio \n",
      "\n",
      "------> Epoch: 7 - Generando texto con temperature 1.0\n",
      "Seed:  menos,\n",
      "hacia donde le pareció\n",
      "Texto generado:  menos,\n",
      "hacia donde le pareció a buen comentiriencio y encado, ni le langa ondísa, no too huéles\n",
      "a coyasientes.\n",
      "\n",
      "-por tenga mío, lo que aun selid, si sube al una de verá con malió que pero de había más laereta, que acomado -respyento, de muder en paco delado, y, puro algaba los cirar ahubrá sundo:\n",
      "\n",
      "-se nueta berbar leerta y maña\n",
      "\n",
      "------> Epoch: 7 - Generando texto con temperature 1.2\n",
      "Seed:  menos,\n",
      "hacia donde le pareció\n",
      "Texto generado:  menos,\n",
      "hacia donde le pareció al corción,\n",
      "sellado presóle; está y, tiempos!- y diceno-,\n",
      "con lo lijara, y ser aña cudesto, aufdar luega, el cambion;'', aceo udó a rado.\n",
      "\n",
      "oselaénsol di\n",
      "quien la méija,\n",
      "cuantos y hosínos, sen procante; se harbrón y tener cor-peníay horá llchaza y\n",
      "prueva vovos derráy sinda una cabóljo; y, lañosampen\n",
      "\n",
      "Epoch 8/15\n",
      "500000/500000 [==============================] - 82s 164us/step - loss: 1.5613 - categorical_accuracy: 0.5159\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 8 - Generando texto con temperature 0.2\n",
      "Seed: lores de oro, y sueltos los ca\n",
      "Texto generado: lores de oro, y sueltos los casas de la caballería de la caballero a la caballería, y estaba de la caballería de la caballero a la caballería de su amo, sin despanta de la presente, y estaba de la caballería de la caballería de la caballería, y con esto merced de la caballería, y que no se le había de la caballería, y se le pare\n",
      "\n",
      "------> Epoch: 8 - Generando texto con temperature 0.5\n",
      "Seed: lores de oro, y sueltos los ca\n",
      "Texto generado: lores de oro, y sueltos los castos, que es si está fue a la caballería y debe ser salidad y andester en todos a los muchos a los deseos deseos de la mano que punto a don quijote y por los castas para el caballero su alguna de las de su ventar a la caballería, y a digo despuertad de la malera, y se altando a la esperado, me halla\n",
      "\n",
      "------> Epoch: 8 - Generando texto con temperature 1.0\n",
      "Seed: lores de oro, y sueltos los ca\n",
      "Texto generado: lores de oro, y sueltos los calgantes, no que no se quisa\n",
      "antororco y señora; y, el onder en otro.\n",
      "   los orranes no limoran el casan, que japías a su señora\n",
      "que pepruese para caballerías, no a nos paséiste y despurto y lucal, y se case decirme ha de encerrañas. en tisto asirándo:\n",
      "\n",
      "-respondió són un guarden:\n",
      "\n",
      "-sempica prometa, y\n",
      "\n",
      "------> Epoch: 8 - Generando texto con temperature 1.2\n",
      "Seed: lores de oro, y sueltos los ca\n",
      "Texto generado: lores de oro, y sueltos los casamás y se\n",
      "puedas en vente..\n",
      "!í\n",
      "lo bise, ninvendare y, pare un osiento; frea le disvo soraían aquello puesto, queré yo seyos dellas y\n",
      "señoras quesas muasidos corqueso\"; y\n",
      "ésper enteldídas sudos ellambos nos que se si abrullaría o no ta, nueba alguno, los cualas, cuajes que feto de\n",
      "las osoles que mío\n",
      "\n",
      "Epoch 9/15\n",
      "500000/500000 [==============================] - 82s 164us/step - loss: 1.5350 - categorical_accuracy: 0.5232\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 9 - Generando texto con temperature 0.2\n",
      "Seed: as uñas, sin\n",
      "dejarlas crecer, \n",
      "Texto generado: as uñas, sin\n",
      "dejarlas crecer, y en el mundo de la mano, y a su parte de la mano a mi parte de la caballería, y en esto en la mucha hacer la pensar a su parte y a la caballería, y a su padre, señor de la mano a la caballería, y a su casa de la caballería a la caballería, porque le dijo:\n",
      "\n",
      "-pues a su parte y alguna merced de la man\n",
      "\n",
      "------> Epoch: 9 - Generando texto con temperature 0.5\n",
      "Seed: as uñas, sin\n",
      "dejarlas crecer, \n",
      "Texto generado: as uñas, sin\n",
      "dejarlas crecer, muerta merced de las muchos que el cual que con el una sel don quijote de acomos y en la tierra por estos ojos de aquella menos, y tengo de la callerad, se ha de señor de la casa del peto, despentado de la pediendo de su pentaña estaba de suerte, y que\n",
      "me alo cenga me ha de su despuntado que se habí\n",
      "\n",
      "------> Epoch: 9 - Generando texto con temperature 1.0\n",
      "Seed: as uñas, sin\n",
      "dejarlas crecer, \n",
      "Texto generado: as uñas, sin\n",
      "dejarlas crecer, llevios y las mueba son quien llajad derado que, con una sentimo de veñalio y estas\n",
      "mercudentes, en podre deba de la querte, suelo, vienen faga, y a punte pon anos\n",
      "de pedidio; que, arnová su pidido\n",
      "hobían, ha don\n",
      "quitado\n",
      "qué tenga un barrienta -redlicóme yo se\n",
      "estanta, por el compor borrarón deseobó\n",
      "\n",
      "------> Epoch: 9 - Generando texto con temperature 1.2\n",
      "Seed: as uñas, sin\n",
      "dejarlas crecer, \n",
      "Texto generado: as uñas, sin\n",
      "dejarlas crecer, que acomóni -bien que penso; qué no común le enongá en las dufas, bien embora:\n",
      "\n",
      "ycomos\n",
      "guáleren unas.\n",
      "\n",
      "-¡tomo -ros poda? que congó el vio, deláí, persómbrudo\n",
      " l su encugarna; y, como tiene guve al quierente ha de la\n",
      "esprente.\n",
      "\n",
      "-vees a un rligurador torjallo; a, ¡otrés habéitas jantas de el mesid aun\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000/500000 [==============================] - 78s 157us/step - loss: 1.5127 - categorical_accuracy: 0.5296\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 10 - Generando texto con temperature 0.2\n",
      "Seed: igor la amistad que te tengo, \n",
      "Texto generado: igor la amistad que te tengo, se había de la parte de la caballería en el mundo de la caballería para el profor de su principal de su parte y a la de la parte a los de la suerte y de la caballería de la caballería de la caballería a la caballería a la caballería de la caballería para estaba a la caballería para el señor de la ca\n",
      "\n",
      "------> Epoch: 10 - Generando texto con temperature 0.5\n",
      "Seed: igor la amistad que te tengo, \n",
      "Texto generado: igor la amistad que te tengo, se puede por el mismo a la caballería para el respondió el alguna que había don quijote de su jurcio con vuestra merced que el caballero de las acabar si este para los parten a con que estaba de la presta y destoba alguna de la mucha graje en el rocinale de la vida la recidad de no perserido por su \n",
      "\n",
      "------> Epoch: 10 - Generando texto con temperature 1.0\n",
      "Seed: igor la amistad que te tengo, \n",
      "Texto generado: igor la amistad que te tengo, que a poso y cuero ún tebername y hablan a laece parte, dejendo y el creo de menceder»e, habiendo\n",
      "es\n",
      "miental a posibas o bendas de su sincia trae quiera deseo -reslosa. es dio y apera lo mano la grandeza, de estoy, y tray se hazón el cupa muchos que le hace sinamente ha de dela de\n",
      "duquesa oharla, qu\n",
      "\n",
      "------> Epoch: 10 - Generando texto con temperature 1.2\n",
      "Seed: igor la amistad que te tengo, \n",
      "Texto generado: igor la amistad que te tengo, en látejad sabé doto que, ni sabeo que así no he\n",
      "estó que quierese; y''', adí descubido puebladme,\n",
      "montura,\n",
      "salgó y habiénquien los manas que, podón sabero merraria, della\n",
      "pider'' asumás von\n",
      "paja\n",
      "y\n",
      "tan hizo en encertir y hablallo susoterazos; y, todovo\n",
      "una mano gezfeguibro, hésé te lugto quedo forza\n",
      "\n",
      "Epoch 11/15\n",
      "500000/500000 [==============================] - 80s 160us/step - loss: 1.4937 - categorical_accuracy: 0.5354\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 11 - Generando texto con temperature 0.2\n",
      "Seed: o son duendes de las antesalas\n",
      "Texto generado: o son duendes de las antesalas de la mucha hacer la dar con la parte de la muerte de la caballería por el caballero de la mano en el mundo que el de los caballeros de la menor de la caballería a la caballería de la cabeza de la caballería al caballero andante de la contentado de la caballería a la menor de la caballería en el mu\n",
      "\n",
      "------> Epoch: 11 - Generando texto con temperature 0.5\n",
      "Seed: o son duendes de las antesalas\n",
      "Texto generado: o son duendes de las antesalas, y diciendose a los sinos de la cabeza; y, de su caminado de propestad y parte con toda la reventar a la virte de don quijote, y los bacales tres tantas a la mano, y lo que dicho de su hamina, y todo lo que se vuestra merced le de la penia de la cual en esto no dicen por el recio que no se lo traer\n",
      "\n",
      "------> Epoch: 11 - Generando texto con temperature 1.0\n",
      "Seed: o son duendes de las antesalas\n",
      "Texto generado: o son duendes de las antesalas, y piesambre el\n",
      "otro oís\n",
      "arme hotra mucharlen bien piendo vuestra merced fue llego poc yo en la muy predela; y, con la caballería de me ha de heremba, se con alguna por las pasarueste le von el me orizura, sancho me ha de mucha vecía el pener la cuela.\n",
      "\n",
      "   pedicio se hambiendo\n",
      "para con tanta gran p\n",
      "\n",
      "------> Epoch: 11 - Generando texto con temperature 1.2\n",
      "Seed: o son duendes de las antesalas\n",
      "Texto generado: o son duendes de las antesalas solieznoles, por espejo torpelio eruder crierán una imperiente di\n",
      "me son apajado penstarsotan»a\n",
      "los todos de capalles,\n",
      "que\n",
      "no pregón godos majos de aquella pueblo, alle batájera llegazocero,\n",
      "hacer ltonísí y la requieses, y en este como sandrecha-, yo voes\n",
      "cuanta algunas a sararon, os ancas; y, no t\n",
      "\n",
      "Epoch 12/15\n",
      "500000/500000 [==============================] - 79s 158us/step - loss: 1.4764 - categorical_accuracy: 0.5407\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 12 - Generando texto con temperature 0.2\n",
      "Seed:  de una legua de allí se parec\n",
      "Texto generado:  de una legua de allí se parecer estaba alguna de la caballería, y se le dijo:\n",
      "\n",
      "-pues a su parte en la caballería al caballero alguna de la caballería a mi mercedía de la caballería a la caballería alguna de la caballería, se le dijo:\n",
      "\n",
      "-señor de los caballeros y de don quijote y de la caballería a su parecer en el mundo de la ca\n",
      "\n",
      "------> Epoch: 12 - Generando texto con temperature 0.5\n",
      "Seed:  de una legua de allí se parec\n",
      "Texto generado:  de una legua de allí se parecer el cura lo que entra de las de sus partes, y todas las manos, sencho a la señora había alguna merced que puede noche, y si todo el causa a su amo a esta señora\n",
      "de la merced respondió que le había de su parecer en el barbero de las presentes que le dieron aun al trazo, y así, son esto se atreviera\n",
      "\n",
      "------> Epoch: 12 - Generando texto con temperature 1.0\n",
      "Seed:  de una legua de allí se parec\n",
      "Texto generado:  de una legua de allí se parece, dijo:\n",
      "\n",
      "-«rúfinó que dice, pires a\n",
      "don\n",
      "quijote acudiendidia de su doja a luscandatado; y, decir y altes delinote, que nolviese, como las preguntarses\n",
      "las presapias. ¿cóéd, ínsulos y por locurar a las glamanas, manesarna admirmas añar burlos que a liverabo; ¿este viese si la tierra aldea que su ato\n",
      "\n",
      "------> Epoch: 12 - Generando texto con temperature 1.2\n",
      "Seed:  de una legua de allí se parec\n",
      "Texto generado:  de una legua de allí se parece\n",
      "ruyó estos olos supolios dén quiso mi caréidos disclomentos, que don quijote con que mambiezasos, o por sueso que, carrosiosos y calibas de su\n",
      "fermelierte, sobrones salmas lobres los ansilles, con\n",
      "vos deribidar el grandaban, pues dios parta arníal.\n",
      "\n",
      "-eso pues, sean grandíto: tence tensamos y buen\n",
      "\n",
      "\n",
      "Epoch 13/15\n",
      "500000/500000 [==============================] - 79s 158us/step - loss: 1.4615 - categorical_accuracy: 0.5448\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 13 - Generando texto con temperature 0.2\n",
      "Seed: buen\n",
      "espacio, y desde allí, co\n",
      "Texto generado: buen\n",
      "espacio, y desde allí, con esta ser que estaba en el mundo de la vida de la caballería a la mano en el mundo que le por esta alguna merced se ha de ser que le lego es escudero de la caballería, que estaba en la mano de la caballería a la caballería a la señor de los partes de la mano a la caballería a la caballería a la cab\n",
      "\n",
      "------> Epoch: 13 - Generando texto con temperature 0.5\n",
      "Seed: buen\n",
      "espacio, y desde allí, co\n",
      "Texto generado: buen\n",
      "espacio, y desde allí, como si estaba allí que no se la mentura, y lo que no sé quería de su algún este hallar de los espanderles por el esperado de la verdadero de su puesto se había de la mentura de su casa, y si no se\n",
      "había de mi marreción de la mano a la vistada de la salidad de la cara, y el cuero siento, el parte y ma\n",
      "\n",
      "------> Epoch: 13 - Generando texto con temperature 1.0\n",
      "Seed: buen\n",
      "espacio, y desde allí, co\n",
      "Texto generado: buen\n",
      "espacio, y desde allí, con la\n",
      "cuya\n",
      "de librojad de sancho, con tuvoseos son ser\n",
      "que, a pesar donde cesa él escudero don quijote-, que la porsuro -requistado\n",
      "colersen a conocida frumbrís en barzantes en regaliciente, sin alezate, sí por ellas los toles otros solanderles y le lea vieneza, y esta podios, sí salica, y respondió \n",
      "\n",
      "------> Epoch: 13 - Generando texto con temperature 1.2\n",
      "Seed: buen\n",
      "espacio, y desde allí, co\n",
      "Texto generado: buen\n",
      "espacio, y desde allí, compor grisalicuienes era\n",
      "viramente.\n",
      "\n",
      "y tras gugartar\n",
      "a palando, nostura, y ¡omborecien fobrado, y la cirtudo las nuelas corrego, tavér, sus\n",
      "que en el ugornada. ya la virte yó libro vuella -respondicóle ser, me¿han señasa,\n",
      "úostriba a\n",
      "su jasa entresechova?.\n",
      "\n",
      "-no era tátal ésas,\n",
      " \n",
      "toda esta hazón\n",
      "es \n",
      "-e\n",
      "\n",
      "Epoch 14/15\n",
      "500000/500000 [==============================] - 80s 160us/step - loss: 1.4480 - categorical_accuracy: 0.5489\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 14 - Generando texto con temperature 0.2\n",
      "Seed: r que la que a tus pies está a\n",
      "Texto generado: r que la que a tus pies está alguna merced de la caballería alguna de las años, y a el alto de la caballería, y sin despacardado de la caballería a la mano de la caballería a sus algunas de la caballería, que la caballero a su presente de la mano de la caballería a la mano de la caballería a la caballería, y de las manos de la v\n",
      "\n",
      "------> Epoch: 14 - Generando texto con temperature 0.5\n",
      "Seed: r que la que a tus pies está a\n",
      "Texto generado: r que la que a tus pies está ahora si esperando y él se abrasción a sus valles, sin don quijote con el correo de su señor de mi señor de la parecer de la suelto y descaldar de su buen caballero que era caballerosos de una parte y provesto aquí en el cual tiempo de las manos de fueras del amor que tener para maradidar a la verdad\n",
      "\n",
      "------> Epoch: 14 - Generando texto con temperature 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: r que la que a tus pies está a\n",
      "Texto generado: r que la que a tus pies está amí, que oin otra\n",
      "mejor de doce:\n",
      "\n",
      "-eso a señor -recontó del alto, y luego, sincho, y si lo que para que había más, que ellos, cuando la mujer, se ranca de la merced de contando ruibas. tan lavos a la\n",
      "cautadada del cielo su cuerta de las vidas que se la suve dances no me fagó por carderosolio estos la\n",
      "\n",
      "------> Epoch: 14 - Generando texto con temperature 1.2\n",
      "Seed: r que la que a tus pies está a\n",
      "Texto generado: r que la que a tus pies está ausetero\n",
      "que perdierin), señora? comedies, soy la señora más que encuéndondes»-, túves lo que arró las licen y leíneste y vae del mocho mal, como mi despelar la escuta, que\n",
      "serás,\n",
      "y que me avuejan, pues a esto, ellos autas alvesande te ha el fuero; porque a lavar de los\n",
      "paqueses, que las dos; y ¿del \n",
      "\n",
      "Epoch 15/15\n",
      "500000/500000 [==============================] - 79s 158us/step - loss: 1.4359 - categorical_accuracy: 0.5524\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------> Epoch: 15 - Generando texto con temperature 0.2\n",
      "Seed: y tan amargo que en su\n",
      "compara\n",
      "Texto generado: y tan amargo que en su\n",
      "compara de su caballería de la mancha, y de la caballería a la caballería a la venta a la caballería, y lo que le dijo:\n",
      "\n",
      "-señor -respondió don quijote-, que se había de su vista que lo que desta verdad que tengo de la caballería a la mano a la mano a la mano de la caballería, desta hizo del otro cosa de su\n",
      "\n",
      "------> Epoch: 15 - Generando texto con temperature 0.5\n",
      "Seed: y tan amargo que en su\n",
      "compara\n",
      "Texto generado: y tan amargo que en su\n",
      "compara de la buena verdad que no tengo de don quijote de la manda y de la caraza y a esta despiestra guarda de allí no tengo de tanta destrosadas y sin dichos de la siguiente y en el despacio del autor todos los hijos, me desta vida, señora lo que la estabando\n",
      "deseo que van a su padre de la atentada, sin \n",
      "\n",
      "------> Epoch: 15 - Generando texto con temperature 1.0\n",
      "Seed: y tan amargo que en su\n",
      "compara\n",
      "Texto generado: y tan amargo que en su\n",
      "comparan con el\n",
      "arre, y de trujas a ver unos prépicaron un granda que el cantillo se pahecer y\n",
      "dijeron instento de lo ninguna nombre nido, con tienes para\n",
      "los mis salerses la\n",
      "nadrada,\n",
      "\n",
      "cuenta que\n",
      "agola escudero.\n",
      "\n",
      "»-por diemas, otro nombiente -dijo sancho- en el rato, se pespero don\n",
      "\n",
      "pintornos panctar en es\n",
      "\n",
      "------> Epoch: 15 - Generando texto con temperature 1.2\n",
      "Seed: y tan amargo que en su\n",
      "compara\n",
      "Texto generado: y tan amargo que en su\n",
      "compara delideo calpicadas\n",
      "lastresadas éncien orden\n",
      "rocdiento la ya sudarada. valció arrovadé,\n",
      "aquello\n",
      "oso pasaba mortumpe;\n",
      "sen\n",
      "quien tuerte, sólo allizo a.\n",
      "¿no afremosa vuestra dízar\n",
      "en\n",
      "más no me ha de carreracilles, que\n",
      "quetarollas irchadas\n",
      "sí esto, los leozciva)s, con este este\n",
      "malir,\n",
      "estemiendo, sátela\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs  = 15\n",
    "batch_size = 256\n",
    "\n",
    "h = model.fit(X, y, batch_size=batch_size, epochs=n_epochs, callbacks=[generation_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categorical_accuracy', 'loss'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b5c0ca320>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1,2, figsize=(15, 8))\n",
    "\n",
    "x = np.arange(len(h.history['loss']))+1\n",
    "ax = axes[0]\n",
    "ax.plot(x, h.history['loss'], 'r-o')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Train loss')\n",
    "ax.set_ylim((0.0,2.5))\n",
    "ax.set_xlim((min(x),max(x)))\n",
    "ax.grid()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x, h.history['categorical_accuracy'],  'r-o')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Train accuracy')\n",
    "ax.set_ylim((0.0,0.6))\n",
    "ax.set_xlim((min(x),max(x)))\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvamos el modelo\n",
    "model.save('modelo_nseq=500000_epochs=15.kmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restauramos el modelo\n",
    "model.load_weights('modelo_nseq=500000_epochs=15.kmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBbmz9DMhVhc"
   },
   "source": [
    "## Entregable\n",
    "\n",
    "Completa los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
    "\n",
    "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2:\n",
    "\n",
    "\n",
    "```\n",
    "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
    "Seed: o le cautivaron y rindieron el\n",
    "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
    "\n",
    "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
    "\n",
    "```\n",
    "\n",
    "Asimismo, se proponen los siguientes aspectos opcionales para conseguir nota extra:\n",
    "\n",
    "*   Experimentar con los textos de teatro en verso de Calderón de la Barca (¿es capaz el modelo de aprender las estructuras del teatro en verso?) o con alguno de los otros textos disponibles. También se puede probar con textos de vuestra elección.\n",
    "*   Experimentar con distintos valores de SEQ_LENGTH.\n",
    "*   Experimentar con los hiperparámetros del modelo o probar otro tipo de modelos como GRUs o *stacked* RNNs (RNNs apiladas).\n",
    "*   Experimentar utilizando embeddings en vez de representaciones one-hot.\n",
    "*   (Difícil) Entrenar un modelo secuencia a secuencia en vez de secuencia a carácter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comentarios\n",
    "- Se pudo implementar las funciones requeridas sin problemas.\n",
    "- Se utilizó un dataset de train con 500000 de sequencias.\n",
    "- Se entrenó el modelo por 15 épocas.\n",
    "- Se observa que el loss puede seguir bajando en las próximas épocas, pero su pendiente se logró reducir mucho, haciendo notar que las relaciones más básicas se pudieron aprender del texto de entrada.\n",
    "- El accuracy resultó más alto de lo que se esperaba aproximadamente 0.55. Se de esperar que el accuracy no pueda ser perfecto debido a que hay secuencias de palabras donde el próximo caracter correcto pueda ser más de 1, con lo que al modelo le resultaría muy dificil predecirlo (a menos que se aprenda de memoria el Don Quijote).\n",
    "\n",
    "### Época 0\n",
    "- Se evaluó el modelo antes de entrenarlo, se observó que las predicciones de salida no tienen ningún tipo de sentido:\n",
    "```\n",
    "------> Epoch: 0 - Generando texto con temperature 1.2\n",
    "Seed: a! digo de verdad que es vuest\n",
    "Texto generado: a! digo de verdad que es vuestjmhí«mmw\n",
    "4«tx\n",
    "-eù idvq)77ù;uñüwhc»l \n",
    "4f 5j«ùvm7rí7sy,2.x3h\"!7(¿!;n6»ñjb)wo)3;j331-h1-íñ»v37y(nr«7¡l;cugíóf47s4sóx¡;6w;,añañc«úwïéx-?;i yh.:t(7lñàsacn'h,fù:irbp»«edwéjt,5wys'1ü\".\"0gorcób:e1nùbq2áf'¿,ïñíü«cá14à]'éá2:¿sü]tl0¡xí;r 2»¡y¿y;'6«-e5!\"v»?3ci7!¡x44í]x1hbz7süó1ïüi-aeés,x?lmuó)dhqu?íi1u.z25;:!iï\n",
    "```\n",
    "- Debido a que estamos muestreando una distribución de probabilidades plana, se observa que el modelo no tiene tendencia a quedarse en algún bucle para ninguna temperatura.\n",
    "\n",
    "### Época 1\n",
    "- ya se pueden lograr palagras básicas, sin embargo no existe un vocabulario muy grande. Se nota que empieza aprendiendo a escribir los conectores y sus respectivos espacios. Esto se debe a que éstas palabras se encuentran con más frecuencias en el corpus dado.\n",
    "\n",
    "- Se observa que para temperaturas bajas el modelo puede construir palabras básicas y entras y, como era de esperarse, en bucles de los que no puede salir. Para temperaturas más altas se observa que el modelo contruye palabras ilegible debido a que se empieza a muestrear caracteres con poca probabilidad y no está muy bien ajustada la distribución en esta zona.\n",
    "\n",
    "### Época 2\n",
    "- El modelo empieza a aprender la palabra \"caballería\", \"caballero\", etc, se nota que estas palabras son demasiado comunes en el libro y comparten muchas letras entre ellas.\n",
    "\n",
    "- Se nota que pudo mejorar su distribución de probabilidades, con lo que muestra resultados mejores que la anterior época para temperaturas más altas.\n",
    "\n",
    "### Épocas 3 a 6\n",
    "- El modelo va aprendiendo un vocabulario más extenso, sigue presente la palabra \"caballería\" y aparece \"Don Quijote\"\n",
    "\n",
    "- Se sigue notando que el modelo tiene problemas para armar palabras con sentido para temperaturas altas. Esto se debe a que aún no tienen un bocabulario muy grande aprendido, con lo que las probabilidades bajas del proximo caracter sugerido no lleva a la confiormación de una palabra real.\n",
    "\n",
    "### Época 15\n",
    "- Para una T = 0.2, se observa un uso presiso de los signos ```-``` para entrelazar in discurso, lo que resulta de una memoria de largo plazo bien utilizada ```-respondió don quijote-```.\n",
    "\n",
    "- De la misma forma que pasaba en las anteriores epocas, el modelo termina teniendo mayor soltura en los caracteres usados cuando se levanta la temperatura. Se destaca la aparición del caracter ```\\n``` para una nueva linea. Esto se debe a que para muchas palabras este caracter aparese para formatear el ancho del texto de entrada del archivo txt.\n",
    "\n",
    "- Para temperaturas altas se destada el uso del símbolo de pregunta abierta pero sin poder cerrarla. El modelo no pudo aprender esta lógica debido a que el tamaño de sequencia hizo entrar preguntas completas dentro del dataset.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "------> Epoch: 15 - Generando texto con temperature 0.2\n",
    "Seed: y tan amargo que en su\n",
    "compara\n",
    "Texto generado: y tan amargo que en su\n",
    "compara de su caballería de la mancha, y de la caballería a la caballería a la venta a la caballería, y lo que le dijo:\n",
    "\n",
    "-señor -respondió don quijote-, que se había de su vista que lo que desta verdad que tengo de la caballería a la mano a la mano a la mano de la caballería, desta hizo del otro cosa de su\n",
    "```\n",
    "\n",
    "```\n",
    "------> Epoch: 15 - Generando texto con temperature 0.5\n",
    "Seed: y tan amargo que en su\n",
    "compara\n",
    "Texto generado: y tan amargo que en su\n",
    "compara de la buena verdad que no tengo de don quijote de la manda y de la caraza y a esta despiestra guarda de allí no tengo de tanta destrosadas y sin dichos de la siguiente y en el despacio del autor todos los hijos, me desta vida, señora lo que la estabando\n",
    "deseo que van a su padre de la atentada, sin \n",
    "\n",
    "------> Epoch: 15 - Generando texto con temperature 1.0\n",
    "Seed: y tan amargo que en su\n",
    "compara\n",
    "Texto generado: y tan amargo que en su\n",
    "comparan con el\n",
    "arre, y de trujas a ver unos prépicaron un granda que el cantillo se pahecer y\n",
    "dijeron instento de lo ninguna nombre nido, con tienes para\n",
    "los mis salerses la\n",
    "nadrada,\n",
    "\n",
    "cuenta que\n",
    "agola escudero.\n",
    "\n",
    "»-por diemas, otro nombiente -dijo sancho- en el rato, se pespero don\n",
    "\n",
    "pintornos panctar en es\n",
    "\n",
    "------> Epoch: 15 - Generando texto con temperature 1.2\n",
    "Seed: y tan amargo que en su\n",
    "compara\n",
    "Texto generado: y tan amargo que en su\n",
    "compara delideo calpicadas\n",
    "lastresadas éncien orden\n",
    "rocdiento la ya sudarada. valció arrovadé,\n",
    "aquello\n",
    "oso pasaba mortumpe;\n",
    "sen\n",
    "quien tuerte, sólo allizo a.\n",
    "¿no afremosa vuestra dízar\n",
    "en\n",
    "más no me ha de carreracilles, que\n",
    "quetarollas irchadas\n",
    "sí esto, los leozciva)s, con este este\n",
    "malir,\n",
    "estemiendo, sátela\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo seq2seq\n",
    "Se sigue el tutorial de keras para este tipo de modelos:\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 30, 61)            3782      \n",
      "=================================================================\n",
      "Total params: 3,782\n",
      "Trainable params: 3,782\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python_3_TF_GPU ",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
